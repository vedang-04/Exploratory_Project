{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfTfWVhqU5wG"
      },
      "source": [
        "This Notebook is for 3D holograms with target intensity specified on a rectangular grid (512x512x3 pixels). Here we use extensively trained models for 512x512x3 and 1024x1024x3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJsqiwFrbcKE"
      },
      "source": [
        "**Step 1** - We import the necessary modules and we clone the Github repository into the working directory of this google colab session."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mvRnZd4cOh3",
        "outputId": "74d34ae1-e96c-4692-a9c2-35f69ac37d80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.8.1\n",
            "  Downloading tensorflow-2.8.1-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (23.5.26)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (3.9.0)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.1)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (16.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (1.15.0)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.1)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.9,>=2.8 (from tensorflow==2.8.1)\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.1)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.8.1) (1.58.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.1) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.1) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.1) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.1) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.1)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.1) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.1) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, keras, tensorboard-data-server, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.1\n",
            "    Uninstalling tensorboard-data-server-0.7.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.1 tensorflow-estimator-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X73qa2e8bchP",
        "outputId": "ce448177-fb5e-4427-f834-4610f15be393"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "!git clone https://github.com/vedang-04/Exploratory_Project\n",
        "os.chdir('Exploratory_Project/DeepCGH')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Exploratory_Project'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 44 (delta 6), reused 11 (delta 1), pack-reused 24\u001b[K\n",
            "Receiving objects: 100% (44/44), 111.17 MiB | 23.23 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "Updating files: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qqhgp4X08ck"
      },
      "source": [
        "Now we can use `deepcgh.DeepCGH_Datasets` to generate the dataset and train a model with it.\n",
        "\n",
        "`DeepCGH_Datasets`: is a module that synthesizes training data set.\n",
        "\n",
        "`DeepCGH`: is the main module that contains the pre-trained model and performs the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJSwXkotgw6C"
      },
      "source": [
        "from deepcgh import DeepCGH_Datasets, DeepCGH\n",
        "from utils import GS3D, display_results, get_propagate"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHyud0xW2Axy"
      },
      "source": [
        "We specify the properties of the training dataset that will be used to train the DeepCGH model. Here we only create a small dataset (N=100) since creating and storing a large dataset is time-consuming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_sWtTS8hPbn"
      },
      "source": [
        "retrain = True\n",
        "coordinates = False\n",
        "\n",
        "data = {'path' : 'DeepCGH_Datasets/Disks',# path to store the data\n",
        "        'shape' : (512, 512, 3),# shape of the holograms. The last dimension determines the number of depth planes\n",
        "        'object_type' : 'Disk',# shape of the object in simulated images, can be disk, square, or line\n",
        "        'object_size' : 10,# has no effect if object type is 'Line'\n",
        "        'object_count' : [27, 48],# number of random objects to be created\n",
        "        'intensity' : [0.2, 1],# the (range of) intensity of each object. If a range is specified, for each object the intensity is randomly determined\n",
        "        'normalize' : True,# if the data is 3D, it'll normalize the intensities from plane to plane (see manuscript fot more info)\n",
        "        'centralized' : False,# avoids putting objects near the edges of the hologram (useful for practical optogenetics applications)\n",
        "        'N' : 100, # number of sample holograms to generate\n",
        "        'train_ratio' : 90/100,# the ratio of N that will be used for training\n",
        "        'compression' : 'GZIP',# tfrecords compression format\n",
        "        'name' : 'target',# name of the dictionary that contains the targets (leave as \"target\" if you're not changing the structure of network input)\n",
        "        }"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXC3FK9Cdg-1"
      },
      "source": [
        "We generate the dataset by calling the `getDataset` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OZ-zpN7CXPT",
        "outputId": "f9f4bae2-30c6-455f-b06b-84f88722a39e"
      },
      "source": [
        "dset = DeepCGH_Datasets(data)\n",
        "dset.getDataset()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Exploratory_Project/DeepCGH/deepcgh.py:72: UserWarning: File does not exist. New dataset will be generated once getDataset is called.\n",
            "  warnings.warn('File does not exist. New dataset will be generated once getDataset is called.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory is:\n",
            "/content/Exploratory_Project/DeepCGH \n",
            "\n",
            "/content/Exploratory_Project/DeepCGH/DeepCGH_Datasets/Disks/Disk_SHP(512, 512, 3)_N100_SZ10_INT[0.2, 1]_Crowd[27, 48]_CNTFalse_Split.tfrecords\n",
            "Generating data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:13<00:00,  7.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuY-IyQCbRY"
      },
      "source": [
        "Here we define the model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1omPQvLIdfVP"
      },
      "source": [
        "model = {'path' : 'DeepCGH_Models/Disks',# the path to the saved model\n",
        "        'plane_distance':0.005,# the physical distance between depth planes when we're doing 3D holography\n",
        "         # physical setup parameters\n",
        "        'wavelength':1e-6,# the wavelength of the laser (both simulations and experiments)\n",
        "        'pixel_size':0.000015,# size of the SLM pixel sizes\n",
        "        'int_factor':16,# the interleaving factor\n",
        "         # CNN model and training parameters\n",
        "        'n_kernels':[ 64, 128, 256],# the number of kernels in the U-Net model (see the manuscript)\n",
        "        'input_name':'target',# name of the input layer in the U-Net model\n",
        "        'output_name':'phi_slm',# name of the output layer\n",
        "        'lr' : 1e-7,# learning rate of the optimizer\n",
        "        'batch_size' : 8,\n",
        "        'epochs' : 1,\n",
        "        'shuffle' : 8,# determine how many samples are going to be shuffled\n",
        "        'token' : '64',# string to be attached to the name of the model to differentiate it from similar models\n",
        "        'max_steps' : 5000 , # maxmimum number of batches/steps to be processed\n",
        "        'quantization': 2,\n",
        "        'focal_point': 1,\n",
        "        }\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3OmA9jW24k8"
      },
      "source": [
        "**Step 2** - We create the DeepCGH module and train it. The code includes extensively trained models for 512x512x3 and 1024x1024x3. For those models, training will begin from the pretrained model and run for one more epoch on the dataset we just generated. In other case,we have to train the model that takes time as it will begin from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP54TQI1rNF_",
        "outputId": "31acbd16-e8b4-4b2d-a3dd-a649f85d7579"
      },
      "source": [
        "dcgh = DeepCGH(data, model)\n",
        "# Training\n",
        "dcgh.train(dset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for trained models in:\n",
            "/content/Exploratory_Project/DeepCGH \n",
            "\n",
            "Model already exists.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/saver.py:1161: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "WARNING:tensorflow:From /content/Exploratory_Project/DeepCGH/deepcgh.py:958: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "WARNING:tensorflow:From /content/Exploratory_Project/DeepCGH/deepcgh.py:958: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr0TosSd3Bx9"
      },
      "source": [
        "**Step 3** - Once the model is trained/loaded, we can start inference. We generate a random sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awF4fbrhuSLE"
      },
      "source": [
        "image = dset.get_randSample()[np.newaxis,...]\n",
        "# making inference is as simple as calling the get_hologram method\n",
        "phase = dcgh.get_hologram(np.zeros_like(image)) # the very first inference takes a long time (a known tensorflow characteristic)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puND5WnWFAJd"
      },
      "source": [
        "We compute the phase and measure the inference time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EURuBMch3sEZ"
      },
      "source": [
        "t0 = time()\n",
        "phase = dcgh.get_hologram(image)\n",
        "t = time() - t0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65JRb66wOf4D"
      },
      "source": [
        "Now we simulate the hologram that this phase will generate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbnUOTwn358U"
      },
      "source": [
        "# reconstruction = get_propagate(data, model)(phase)\n",
        "# display_results(image, phase, reconstruction, t)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT0eiVqkKrhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808e82d7-999e-42cd-c260-ec0e9f5e787c"
      },
      "source": [
        "phase"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 1.0471976],\n",
              "         [ 3.1415927],\n",
              "         [-1.0471976],\n",
              "         ...,\n",
              "         [-3.1415927],\n",
              "         [ 1.0471976],\n",
              "         [-3.1415927]],\n",
              "\n",
              "        [[-3.1415927],\n",
              "         [ 1.0471976],\n",
              "         [-1.0471976],\n",
              "         ...,\n",
              "         [-3.1415927],\n",
              "         [ 3.1415927],\n",
              "         [-1.0471976]],\n",
              "\n",
              "        [[-1.0471976],\n",
              "         [-3.1415927],\n",
              "         [ 1.0471976],\n",
              "         ...,\n",
              "         [ 1.0471976],\n",
              "         [-3.1415927],\n",
              "         [ 1.0471976]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-1.0471976],\n",
              "         [ 1.0471976],\n",
              "         [-1.0471976],\n",
              "         ...,\n",
              "         [ 1.0471976],\n",
              "         [ 1.0471976],\n",
              "         [ 3.1415927]],\n",
              "\n",
              "        [[ 1.0471976],\n",
              "         [-1.0471976],\n",
              "         [ 3.1415927],\n",
              "         ...,\n",
              "         [ 1.0471976],\n",
              "         [-1.0471976],\n",
              "         [ 3.1415927]],\n",
              "\n",
              "        [[ 3.1415927],\n",
              "         [ 1.0471976],\n",
              "         [-3.1415927],\n",
              "         ...,\n",
              "         [-3.1415927],\n",
              "         [ 1.0471976],\n",
              "         [-1.0471976]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9Q-53aAjUbL",
        "outputId": "07771c95-70df-47b4-bc92-1950ef2a37d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGepjI0bjXgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}